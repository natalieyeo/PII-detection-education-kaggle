{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5f6adb-ccc5-4043-9023-1f88925bab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from bisect import bisect_left\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import keras\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "from keras import ops\n",
    "from keras.utils import pad_sequences\n",
    "from conlleval import evaluate\n",
    "import sklearn\n",
    "from keras import layers\n",
    "import keras_nlp\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "%run nlp-functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d239ebd-613a-4130-905e-f6d9f407549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_json('data/train.json')\n",
    "#test = pd.read_json('data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9beca6de-0d23-454d-a7c8-18647c66162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('initial_predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931d1e8d-d057-4799-9667-2148740892e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>token_text</th>\n",
       "      <th>correct_label</th>\n",
       "      <th>preds</th>\n",
       "      <th>cm</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>labels_in_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Design</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Thinking</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>innovation</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document  token  token_text correct_label preds  cm  \\\n",
       "0         7      0      Design             O     O  TN   \n",
       "1         7      1    Thinking             O     O  TN   \n",
       "2         7      2         for             O     O  TN   \n",
       "3         7      3  innovation             O     O  TN   \n",
       "4         7      4   reflexion             O     O  TN   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0  Design Thinking for innovation reflexion-Avril...   \n",
       "1  Design Thinking for innovation reflexion-Avril...   \n",
       "2  Design Thinking for innovation reflexion-Avril...   \n",
       "3  Design Thinking for innovation reflexion-Avril...   \n",
       "4  Design Thinking for innovation reflexion-Avril...   \n",
       "\n",
       "                                  tokenized_sentence  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Design, Thinking, for, innovation, reflexion,...   \n",
       "2  [Design, Thinking, for, innovation, reflexion,...   \n",
       "3  [Design, Thinking, for, innovation, reflexion,...   \n",
       "4  [Design, Thinking, for, innovation, reflexion,...   \n",
       "\n",
       "                                  labels_in_sentence  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "2  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "3  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d1d9f-4bb0-4ddb-bc65-58d12304fd7e",
   "metadata": {},
   "source": [
    "The dataset df1 contains the results of the Presidio Analyzer Function on each individual token in Learning Agency PII dataset. Each row contains the document number, word token number, word token text, correct label, Presidio Analyzer prediction, and classification as True Negative, True Positive, False Positive or False Negative for the prediction. We also included three columns containing the sentence context of each word.\n",
    "\n",
    "In the next section, we are going to divide this initial dataset into True Negative and everything else. We will use these Presidio Analyzer results - what the previous model got right and wrong to better balance classes in the data sample for the neural network to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb1ec44-abd8-408b-a786-c3edf09de6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208248, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the indices in each category\n",
    "idx = df1.loc[(df1.cm.isin(['TP','FN','FP'])) | df1.cm.isna(),'sentence_text'].drop_duplicates().index\n",
    "idx_tn = df1.loc[df1.cm == 'TN', 'sentence_text'].drop_duplicates().index\n",
    "\n",
    "df2 = pd.concat([df1.iloc[idx,:],df1.iloc[idx_tn,:]],axis = 0).reset_index(drop = True)\n",
    "total_idx = df2.loc[:,'sentence_text'].drop_duplicates().index\n",
    "df2 = df2.iloc[total_idx,:]\n",
    "tn_df = df2.loc[df2.cm == 'TN',:]\n",
    "\n",
    "#To have more balanced classes, get a random sample of sentences that have all TN tokens and combine this with everything in df2 with a label\n",
    "tn_sample_size = 4000\n",
    "tn_sample = tn_df.sample(tn_sample_size,random_state = 42)\n",
    "df3 = pd.concat([df2.loc[df2.cm != 'TN',:],\n",
    "                 tn_sample],axis = 0).reset_index(drop = True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7722e73-53b1-4c88-abd2-76344039ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8283, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape\n",
    "#df3 has 8283 unique sentences containing all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17439ee9-018a-4df5-9c28-63ccffc0aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cm\n",
       "TN    4000\n",
       "FP    3011\n",
       "TP    1049\n",
       "FN     221\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.cm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0aa589-ed4b-4f2d-9e27-407345ca6350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-NAME_STUDENT', 3: 'I-NAME_STUDENT', 4: 'B-EMAIL', 5: 'I-EMAIL', 6: 'B-URL_PERSONAL', 7: 'I-URL_PERSONAL', 8: 'B-ID_NUM', 9: 'I-ID_NUM', 10: 'B-USERNAME', 11: 'I-USERNAME', 12: 'B-PHONE_NUM', 13: 'I-PHONE_NUM', 14: 'B-STREET_ADDRESS', 15: 'I-STREET_ADDRESS'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"NAME_STUDENT\", \"EMAIL\", \"URL_PERSONAL\", \"ID_NUM\",'USERNAME','PHONE_NUM','STREET_ADDRESS']\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = ['[PAD]',\"O\"] + all_labels\n",
    "    return dict(zip(all_labels,range(0, len(all_labels) + 1)))\n",
    "\n",
    "encoding = make_tag_lookup_table()\n",
    "mapping = dict([(value, key) for key, value in encoding.items()])\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d693feca-67aa-4f29-9410-892a800fc3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53985\n"
     ]
    }
   ],
   "source": [
    "all_tokens = train.tokens.explode().reset_index(drop = True).unique()\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 50000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "del all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de1d2a-8818-4c93-8974-cb9c74ebb240",
   "metadata": {},
   "source": [
    "1. Take a sample of the TN to balance the positive and negative classes.\n",
    "2. Get a label for each word, pad and tokenize the sentences, get a label for each label of words in the sentence.\n",
    "3. Try to run the transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b83ddd-ace2-4dbb-84c4-590886ac777f",
   "metadata": {},
   "source": [
    "**Named Entity Recognition Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "799defe3-6904-4207-91ab-1b4b983b7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5530ea3e-1c24-4f55-b4a2-82447722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4680446c-4427-4a98-b136-6cf7ac17a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=3298, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9bff2729-a468-4e6e-bb16-cb279dfa24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=None\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = ops.cast((y_true > 0), dtype=\"float32\")\n",
    "        loss = loss * mask\n",
    "        return ops.sum(loss) / ops.sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0eb613f3-9823-43c8-a32a-7c8815a19279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(dataset,model, beta = 5):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = model.predict(x, verbose=0)\n",
    "        predictions = ops.argmax(output, axis=-1)\n",
    "        predictions = ops.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = ops.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    print(f'processed {len(predicted_tags)} tokens')\n",
    "    non_o = 0\n",
    "    tn = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    misclassified = 0\n",
    "    for i in range(len(real_tags)):\n",
    "        if real_tags[i] != \"O\":\n",
    "            non_o += 1\n",
    "            if real_tags[i] == predicted_tags[i]:\n",
    "                #if real tag equals predicted tag and does not equal O\n",
    "                tp += 1\n",
    "            else:\n",
    "                if predicted_tags[i] != \"O\":\n",
    "                    #if real tag is not O and predicted tag is not O but does not match real tag\n",
    "                    misclassified += 1\n",
    "                if predicted_tags[i] == \"O\":\n",
    "                    #if real tag is not O and predicted tag is O\n",
    "                    fn += 1\n",
    "        else:\n",
    "            if predicted_tags[i] == \"O\":\n",
    "                #if real tag is O and predicted tag is O\n",
    "                tn += 1\n",
    "            else:\n",
    "                #if real tag is O and predicted tag is not O\n",
    "                fp += 1\n",
    "\n",
    "    accuracy_non_o = tp / non_o\n",
    "    accuracy_score = sklearn.metrics.accuracy_score(real_tags, predicted_tags)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall / (precision + recall))\n",
    "    s_micro = (1+(beta**2))*tp/(((1+(beta**2))*tp) + ((beta**2)*fn) + fp)\n",
    "\n",
    "    print(f'True Positive: {tp}, True Negative: {tn}, False Positive: {fp}, False Negative: {fn}')\n",
    "    print(f'{misclassified} tokens identified as PII but mislabelled.')\n",
    "    print(f'Accuracy (non-O): {accuracy_non_o}')\n",
    "    print(f'Accuracy: {accuracy_score}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 score: {f1}')\n",
    "    print(f'S-micro score: {s_micro}')\n",
    "    return real_tags, predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4a53ef03-d6b4-45c7-8ba5-ffe78a79ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x[1] for x in list(mapping.items())][2:]\n",
    "def calculate_metrics_by_label(y_true, y_pred,labels, beta = 5):\n",
    "    cm = sklearn.metrics.multilabel_confusion_matrix(real,preds, labels = labels)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    s_micro = []\n",
    "    total_tp = 0\n",
    "    total_fn = 0\n",
    "    total_fp = 0\n",
    "    for i in range(len(labels)):\n",
    "        tn = cm[i][0][0]\n",
    "        fp = cm[i][0][1]\n",
    "        total_fp += fp\n",
    "        fn = cm[i][1][0]\n",
    "        total_fn += fn\n",
    "        tp = cm[i][1][1]\n",
    "        total_tp += tp\n",
    "        p = tp / (tp + fp)\n",
    "        r = tp / (tp + fn)\n",
    "        f = 2 * (p * r / (p + r))\n",
    "        s = (1+(beta**2))*tp/(((1+(beta**2))*tp) + ((beta**2)*fn) + fp)\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "        s_micro.append(s)\n",
    "    df = pd.DataFrame({'labels' : labels,\n",
    "                        'precision': precision,\n",
    "                        'recall' : recall,\n",
    "                        'f1 score' : f1,\n",
    "                        's_micro' : s_micro})\n",
    "    print(f'True Positive: {total_tp}, False Positive: {total_fp}, False Negative: {total_fn}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26538b6-3aaa-4aa5-baef-d664d3d5d564",
   "metadata": {},
   "source": [
    "**Running the model on a subset of sentences**\n",
    "\n",
    "Here, we are selecting sentences in df3, the dataset that contains all true positives, false negatives, false positives, and a sample of the true negative tokens from the results of Microsoft Presidio with rule-based NER. Since sentences do contain many negative labels in addition to the positive labels, we have removed duplicates from the dataset to prevent any overlap. We have also included only a small subset of the the sentences containing all true negatives to balance the classes and improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65dd16-5e0c-4c33-9b85-b04e44a0d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df3['tokenized_sentence'],df3['labels_by_sentence'],test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train = pad_sequences(X_train.map(lookup_layer))\n",
    "X_test = pad_sequences(X_test.map(lookup_layer))\n",
    "y_train = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_train]))\n",
    "y_test = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_test]))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model.compile(optimizer='adam',loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=5)\n",
    "\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    train.loc[345,'sentence_tokens'][0]\n",
    ")\n",
    "sample_input = ops.reshape(sample_input, newshape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c6027f6e-5f2e-47f1-b04a-de114cd7064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 39483 tokens\n",
      "True Positive: 272, True Negative: 38385, False Positive: 641, False Negative: 91\n",
      "94 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.5951859956236324\n",
      "Accuracy: 0.9790796038801509\n",
      "Precision: 0.29791894852135814\n",
      "Recall: 0.7493112947658402\n",
      "F1 score: 0.426332288401254\n",
      "S-micro score: 0.7080496595915098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:44:11.737679: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e8f6f69a-8acf-446e-9c49-58aa1c4078e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 272, False Positive: 735, False Negative: 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_1813/587648281.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p = tp / (tp + fp)\n",
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_1813/587648281.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  r = tp / (tp + fn)\n",
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_1813/587648281.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  s = (1+(beta**2))*tp/(((1+(beta**2))*tp) + ((beta**2)*fn) + fp)\n"
     ]
    }
   ],
   "source": [
    "results = calculate_metrics_by_label(real, preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a8f0f9de-7326-4b13-a557-4eb788a2692d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>s_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.358674</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.488712</td>\n",
       "      <td>0.734531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.170082</td>\n",
       "      <td>0.453552</td>\n",
       "      <td>0.247392</td>\n",
       "      <td>0.426230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-EMAIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-EMAIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.141049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-URL_PERSONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-ID_NUM</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.205534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-ID_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-USERNAME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-USERNAME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B-PHONE_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B-STREET_ADDRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-STREET_ADDRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              labels  precision    recall  f1 score   s_micro\n",
       "0     B-NAME_STUDENT   0.358674  0.766667  0.488712  0.734531\n",
       "1     I-NAME_STUDENT   0.170082  0.453552  0.247392  0.426230\n",
       "2            B-EMAIL        NaN  0.000000       NaN  0.000000\n",
       "3            I-EMAIL        NaN       NaN       NaN       NaN\n",
       "4     B-URL_PERSONAL   1.000000  0.136364  0.240000  0.141049\n",
       "5     I-URL_PERSONAL        NaN       NaN       NaN       NaN\n",
       "6           B-ID_NUM   0.666667  0.200000  0.307692  0.205534\n",
       "7           I-ID_NUM        NaN       NaN       NaN       NaN\n",
       "8         B-USERNAME        NaN       NaN       NaN       NaN\n",
       "9         I-USERNAME        NaN       NaN       NaN       NaN\n",
       "10       B-PHONE_NUM        NaN       NaN       NaN       NaN\n",
       "11       I-PHONE_NUM        NaN       NaN       NaN       NaN\n",
       "12  B-STREET_ADDRESS        NaN       NaN       NaN       NaN\n",
       "13  I-STREET_ADDRESS        NaN       NaN       NaN       NaN"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "52e86d31-7860-4a44-acdd-3a130812807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:45:14.071143: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 153927 tokens\n",
      "True Positive: 1728, True Negative: 151955, False Positive: 37, False Negative: 40\n",
      "167 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.8930232558139535\n",
      "Accuracy: 0.9984148330052557\n",
      "Precision: 0.9790368271954675\n",
      "Recall: 0.9773755656108597\n",
      "F1 score: 0.9782054910840644\n",
      "S-micro score: 0.9774393560317634\n"
     ]
    }
   ],
   "source": [
    "true_y_train, preds_train = calculate_metrics(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a32a1-7716-49ee-afc0-521062eb93a3",
   "metadata": {},
   "source": [
    "**Using the full text instead of the sentence context for training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5c7dbc3e-44d1-4517-8c09-e03c5407f3ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1402s\u001b[0m 8s/step - loss: 0.1482\n",
      "Epoch 2/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1367s\u001b[0m 8s/step - loss: 0.0064\n",
      "Epoch 3/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1315s\u001b[0m 8s/step - loss: 0.0044\n",
      "Epoch 4/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1325s\u001b[0m 8s/step - loss: 0.0017\n",
      "Epoch 5/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1336s\u001b[0m 8s/step - loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3f79c1450>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['tokens'],train['labels'],test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train = pad_sequences(X_train.map(lookup_layer))\n",
    "X_test = pad_sequences(X_test.map(lookup_layer))\n",
    "y_train = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_train]))\n",
    "y_test = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_test]))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model.compile(optimizer='adam',loss=loss)\n",
    "\n",
    "ner_model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b568eb-08ac-4b0f-9b8c-98ac608eeb13",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "loss: 0.1482\n",
    "Epoch 2/5\n",
    "loss: 0.0064\n",
    "Epoch 3/5\n",
    "loss: 0.0044\n",
    "Epoch 4/5\n",
    "loss: 0.0017\n",
    "Epoch 5/5\n",
    "loss: 0.0014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2731d7a9-a54c-40ce-9ed8-2acaf6717817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 02:31:25.141757: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1005699 tokens\n",
      "True Positive: 107, True Negative: 1004925, False Positive: 210, False Negative: 416\n",
      "41 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.18971631205673758\n",
      "Accuracy: 0.9993367796925322\n",
      "Precision: 0.33753943217665616\n",
      "Recall: 0.2045889101338432\n",
      "F1 score: 0.12738095238095237\n",
      "S-micro score: 0.20773596176821985\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c786d457-082e-410e-aecc-dfefe801393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 107, False Positive: 251, False Negative: 457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_1813/1002851054.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p = tp / (tp + fp)\n",
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_1813/1002851054.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  r = tp / (tp + fn)\n",
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_1813/1002851054.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  s = (1+(beta**2))*tp/(((1+(beta**2))*tp) + ((beta**2)*fn) + fp)\n"
     ]
    }
   ],
   "source": [
    "results = calculate_metrics_by_label(real, preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f07c7815-2872-4467-a496-8bbd9079e63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>s_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.266160</td>\n",
       "      <td>0.139721</td>\n",
       "      <td>0.267136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>0.101648</td>\n",
       "      <td>0.154662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-EMAIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-EMAIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-URL_PERSONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-ID_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-ID_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-USERNAME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-USERNAME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B-PHONE_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B-STREET_ADDRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-STREET_ADDRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              labels  precision    recall  f1 score   s_micro\n",
       "0     B-NAME_STUDENT   0.294118  0.266160  0.139721  0.267136\n",
       "1     I-NAME_STUDENT   0.308333  0.151639  0.101648  0.154662\n",
       "2            B-EMAIL        NaN  0.000000       NaN  0.000000\n",
       "3            I-EMAIL        NaN       NaN       NaN       NaN\n",
       "4     B-URL_PERSONAL        NaN  0.000000       NaN  0.000000\n",
       "5     I-URL_PERSONAL        NaN       NaN       NaN       NaN\n",
       "6           B-ID_NUM        NaN  0.000000       NaN  0.000000\n",
       "7           I-ID_NUM        NaN       NaN       NaN       NaN\n",
       "8         B-USERNAME        NaN       NaN       NaN       NaN\n",
       "9         I-USERNAME        NaN       NaN       NaN       NaN\n",
       "10       B-PHONE_NUM        NaN  0.000000       NaN  0.000000\n",
       "11       I-PHONE_NUM        NaN  0.000000       NaN  0.000000\n",
       "12  B-STREET_ADDRESS        NaN  0.000000       NaN  0.000000\n",
       "13  I-STREET_ADDRESS        NaN  0.000000       NaN  0.000000"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb4826-9cc4-40b0-a414-2c7f0cae696d",
   "metadata": {},
   "source": [
    "This model is highly overfit. With such imbalanced classes, it only predicted 107 true positive labels with 457 mislabelled tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00f4c5b6-e4f1-4887-a522-57e4151ad085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create document with unique sentence labels\n",
    "unique_sent_idx = df1[['document','unique_sentence_test']].drop_duplicates().index\n",
    "sentence_df = df1.loc[unique_sent_idx,['document','tokenized_sentence','labels_by_sentence','unique_sentence_test']]\n",
    "del unique_sent_idx\n",
    "sentence_df[['unique_sentence_test','document']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a2c571-55d1-4074-8b0d-e3fe1e13c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac6b753-5462-4a11-8850-9605ce8685a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.drop('unique_sentence_test',axis = 1,inplace = True)\n",
    "sentence_df['sentence'] = sentence_df[['tokenized_sentence']].map(lambda x: \" \".join(x))\n",
    "sentence_df['word_labels'] = sentence_df[['labels_by_sentence']].map(lambda x: \",\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d99ace52-3b44-4b76-ab3a-e4890d2c6d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>labels_by_sentence</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "      <td>Design Thinking for innovation reflexion - Avr...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,B-NAME_STUDENT,I-NAME_STUDENT,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>[Challenge, &amp;, selection, \\n\\n]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>Challenge &amp; selection \\n\\n</td>\n",
       "      <td>O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>[The, tool, I, use, to, help, all, stakeholder...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>The tool I use to help all stakeholders findin...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7</td>\n",
       "      <td>[What, exactly, is, a, mind, map, ?]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>What exactly is a mind map ?</td>\n",
       "      <td>O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7</td>\n",
       "      <td>[According, to, the, definition, of, Buzan, T....</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>According to the definition of Buzan T. and Bu...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    document                                 tokenized_sentence  \\\n",
       "0          7  [Design, Thinking, for, innovation, reflexion,...   \n",
       "12         7                    [Challenge, &, selection, \\n\\n]   \n",
       "16         7  [The, tool, I, use, to, help, all, stakeholder...   \n",
       "40         7               [What, exactly, is, a, mind, map, ?]   \n",
       "47         7  [According, to, the, definition, of, Buzan, T....   \n",
       "\n",
       "                                   labels_by_sentence  \\\n",
       "0   [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...   \n",
       "12                                       [O, O, O, O]   \n",
       "16  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "40                              [O, O, O, O, O, O, O]   \n",
       "47                     [O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                             sentence  \\\n",
       "0   Design Thinking for innovation reflexion - Avr...   \n",
       "12                         Challenge & selection \\n\\n   \n",
       "16  The tool I use to help all stakeholders findin...   \n",
       "40                       What exactly is a mind map ?   \n",
       "47  According to the definition of Buzan T. and Bu...   \n",
       "\n",
       "                                          word_labels  \n",
       "0   O,O,O,O,O,O,O,O,O,B-NAME_STUDENT,I-NAME_STUDENT,O  \n",
       "12                                            O,O,O,O  \n",
       "16    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
       "40                                      O,O,O,O,O,O,O  \n",
       "47                                O,O,O,O,O,O,O,O,O,O  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "111da70a-a85f-430e-b0c0-124517f1e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockWithLSTM(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Bidirectional(layers.LSTM(ff_dim, return_sequences=True)),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "938176c3-c819-4832-ac83-e704e0151ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModelWithLSTM(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=3298, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlockWithLSTM(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f5c8ae19-8b0b-4a0e-a7e5-68883a171656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df3['tokenized_sentence'],df3['labels_by_sentence'],test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train = pad_sequences(X_train.map(lookup_layer))\n",
    "X_test = pad_sequences(X_test.map(lookup_layer))\n",
    "y_train = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_train]))\n",
    "y_test = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_test]))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e95f1a8-36de-4b7c-8d88-c54aa37edf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 116ms/step - loss: 0.2471\n",
      "Epoch 2/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 109ms/step - loss: 0.0434\n",
      "Epoch 3/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 108ms/step - loss: 0.0175\n",
      "Epoch 4/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 117ms/step - loss: 0.0105\n",
      "Epoch 5/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 110ms/step - loss: 0.0058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x33e279d90>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model_lstm = NERModelWithLSTM(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model_lstm.compile(optimizer='adam',loss=loss)\n",
    "ner_model_lstm.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "751db584-27d3-4429-8ce5-13fde344cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 39483 tokens\n",
      "True Positive: 260, True Negative: 38812, False Positive: 214, False Negative: 156\n",
      "41 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.5689277899343544\n",
      "Accuracy: 0.9895904566522301\n",
      "Precision: 0.5485232067510548\n",
      "Recall: 0.625\n",
      "F1 score: 0.5842696629213483\n",
      "S-micro score: 0.6216663601250689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 22:50:38.940457: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset,ner_model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49658e71-46f7-47ba-be5f-ff0f2e340256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 260, False Positive: 255, False Negative: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_11203/2515878672.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p = tp / (tp + fp)\n",
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_11203/2515878672.py:20: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  r = tp / (tp + fn)\n",
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_11203/2515878672.py:22: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  s = (1+(beta**2))*tp/(((1+(beta**2))*tp) + ((beta**2)*fn) + fp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>s_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.501326</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.612642</td>\n",
       "      <td>0.770582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.507353</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.432602</td>\n",
       "      <td>0.380811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-EMAIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-EMAIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.047187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-URL_PERSONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-ID_NUM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.103586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-ID_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-USERNAME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-USERNAME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B-PHONE_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B-STREET_ADDRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-STREET_ADDRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              labels  precision    recall  f1 score   s_micro\n",
       "0     B-NAME_STUDENT   0.501326  0.787500  0.612642  0.770582\n",
       "1     I-NAME_STUDENT   0.507353  0.377049  0.432602  0.380811\n",
       "2            B-EMAIL        NaN  0.000000       NaN  0.000000\n",
       "3            I-EMAIL        NaN       NaN       NaN       NaN\n",
       "4     B-URL_PERSONAL   1.000000  0.045455  0.086957  0.047187\n",
       "5     I-URL_PERSONAL        NaN       NaN       NaN       NaN\n",
       "6           B-ID_NUM   1.000000  0.100000  0.181818  0.103586\n",
       "7           I-ID_NUM        NaN       NaN       NaN       NaN\n",
       "8         B-USERNAME        NaN       NaN       NaN       NaN\n",
       "9         I-USERNAME        NaN       NaN       NaN       NaN\n",
       "10       B-PHONE_NUM        NaN       NaN       NaN       NaN\n",
       "11       I-PHONE_NUM        NaN       NaN       NaN       NaN\n",
       "12  B-STREET_ADDRESS        NaN       NaN       NaN       NaN\n",
       "13  I-STREET_ADDRESS        NaN       NaN       NaN       NaN"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = calculate_metrics_by_label(real, preds, labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346554b-7dc2-4e0e-a06e-ba57a9f21d35",
   "metadata": {},
   "source": [
    "Testing models on a different sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4ffe07ea-2481-4a69-9797-4cfd8f30d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df3['tokenized_sentence'],df3['labels_by_sentence'],test_size = 0.2, random_state = 50)\n",
    "\n",
    "X_train = pad_sequences(X_train.map(lookup_layer))\n",
    "X_test = pad_sequences(X_test.map(lookup_layer))\n",
    "y_train = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_train]))\n",
    "y_test = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_test]))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1ea2ee70-11b9-4c1e-85cb-423b975d01b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 120ms/step - loss: 0.4341\n",
      "Epoch 2/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 114ms/step - loss: 0.0700\n",
      "Epoch 3/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 116ms/step - loss: 0.0638\n",
      "Epoch 4/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 116ms/step - loss: 0.0601\n",
      "Epoch 5/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 117ms/step - loss: 0.0501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4958d0390>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model_lstm = NERModelWithLSTM(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model_lstm.compile(optimizer='adam',loss=loss)\n",
    "ner_model_lstm.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "12f91007-da91-41c7-9601-5f8995f6c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 38773 tokens\n",
      "True Positive: 90, True Negative: 38241, False Positive: 45, False Negative: 377\n",
      "20 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.18480492813141683\n",
      "Accuracy: 0.9886003146519485\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.19271948608137046\n",
      "F1 score: 0.29900332225913623\n",
      "S-micro score: 0.19813717188823032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 00:12:03.738409: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset,ner_model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "485a61ca-46ef-42bb-8a86-e597242e6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.4274\n",
      "Epoch 2/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - loss: 0.0760\n",
      "Epoch 3/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - loss: 0.0322\n",
      "Epoch 4/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 51ms/step - loss: 0.0217\n",
      "Epoch 5/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - loss: 0.0167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4d745b1d0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model.compile(optimizer='adam',loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a7e874de-9eb4-45e8-a6d5-812fb0c10e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 38773 tokens\n",
      "True Positive: 283, True Negative: 37802, False Positive: 484, False Negative: 77\n",
      "127 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.5811088295687885\n",
      "Accuracy: 0.9822556933948882\n",
      "Precision: 0.36897001303780963\n",
      "Recall: 0.7861111111111111\n",
      "F1 score: 0.5022182786157942\n",
      "S-micro score: 0.7533531278795945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 00:17:26.542172: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset,ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f6d5b-3760-4e80-be80-c6cecf55f804",
   "metadata": {},
   "source": [
    "Can we reproduce the random_state = 42 performance of the NER with LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fc23a354-ad98-4e2f-8443-c71748f8bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 128ms/step - loss: 0.3360\n",
      "Epoch 2/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 117ms/step - loss: 0.0565\n",
      "Epoch 3/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 117ms/step - loss: 0.0262\n",
      "Epoch 4/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 117ms/step - loss: 0.0115\n",
      "Epoch 5/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 118ms/step - loss: 0.0067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x45a92cf50>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = get_training_data(df3['tokenized_sentence'], df3['labels_by_sentence'])\n",
    "\n",
    "ner_model_lstm = NERModelWithLSTM(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model_lstm.compile(optimizer='adam',loss=loss)\n",
    "ner_model_lstm.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fe84d967-07a4-4071-95eb-2a8242977ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 39483 tokens\n",
      "True Positive: 164, True Negative: 38950, False Positive: 76, False Negative: 265\n",
      "28 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.3588621444201313\n",
      "Accuracy: 0.9906542056074766\n",
      "Precision: 0.6833333333333333\n",
      "Recall: 0.3822843822843823\n",
      "F1 score: 0.49028400597907323\n",
      "S-micro score: 0.3888736890104879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 00:40:06.232222: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset,ner_model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "807b590e-d562-4af8-80c8-2e8ef0380070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 67ms/step - loss: 0.3491\n",
      "Epoch 2/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - loss: 0.0656\n",
      "Epoch 3/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - loss: 0.0340\n",
      "Epoch 4/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - loss: 0.0252\n",
      "Epoch 5/5\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - loss: 0.0238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4b5c9c9d0>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model.compile(optimizer='adam',loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4e2b167e-8e34-4610-aa2d-ab85029309f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 39483 tokens\n",
      "True Positive: 197, True Negative: 38815, False Positive: 211, False Negative: 210\n",
      "50 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.4310722100656455\n",
      "Accuracy: 0.9880708152875921\n",
      "Precision: 0.48284313725490197\n",
      "Recall: 0.48402948402948404\n",
      "F1 score: 0.4834355828220859\n",
      "S-micro score: 0.48398374751960693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 00:44:08.645165: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset,ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d3288-1c43-4756-9892-a2e02133216d",
   "metadata": {},
   "source": [
    "Fine-tune batch size, number of transformer heads, embed_dims, ff_dims, number of epochs\n",
    "\n",
    "Figure out what sample to use - try measuring model accuracy with all the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e63646-f7fd-49f2-83ad-66e8e8358c8f",
   "metadata": {},
   "source": [
    "Running the model on all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4291ef43-8669-49ac-8ad3-d36d9b3218ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated sentences: 0\n"
     ]
    }
   ],
   "source": [
    "#Create document with unique sentence labels\n",
    "unique_sent_idx = df1[['document','sentence']].drop_duplicates().index\n",
    "sentence_df = df1.loc[unique_sent_idx,['document','tokenized_sentence','labels_by_sentence','sentence']]\n",
    "del unique_sent_idx\n",
    "print(f\"Number of duplicated sentences: {sentence_df[['sentence','document']].duplicated().sum()}\")\n",
    "sentence_df['word_labels'] = sentence_df[['labels_by_sentence']].map(lambda x: \",\".join(x))\n",
    "sentence_df.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cb59f896-cd10-4278-9414-b9eb4e3a17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "documents = sentence_df.document.unique()\n",
    "random_sample = np.random.choice(documents, size = 1000, replace = False)\n",
    "sample_data = sentence_df.loc[sentence_df.document.isin(list(random_sample)),['tokenized_sentence','labels_by_sentence']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_data['tokenized_sentence'], \n",
    "                                                    sample_data['labels_by_sentence'],\n",
    "                                                    test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train = pad_sequences(X_train.map(lookup_layer))\n",
    "X_test = pad_sequences(X_test.map(lookup_layer))\n",
    "y_train = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_train]))\n",
    "y_test = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_test]))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fc4c7b58-55da-4650-8127-fc3567d8e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 115ms/step - loss: 0.0862\n",
      "Epoch 2/5\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 116ms/step - loss: 0.0042\n",
      "Epoch 3/5\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 118ms/step - loss: 0.0036\n",
      "Epoch 4/5\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 118ms/step - loss: 0.0013\n",
      "Epoch 5/5\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 118ms/step - loss: 5.4111e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4d216c9d0>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model_lstm = NERModelWithLSTM(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model_lstm.compile(optimizer='adam',loss=loss)\n",
    "ner_model_lstm.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e595260-51f2-4255-89a4-d1f155dc037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 23:55:36.882047: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 147792 tokens\n",
      "True Positive: 12, True Negative: 147706, False Positive: 1, False Negative: 72\n",
      "1 tokens identified as PII but mislabelled.\n",
      "Accuracy (non-O): 0.1411764705882353\n",
      "Accuracy: 0.9994992963083252\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.14285714285714285\n",
      "F1 score: 0.24742268041237114\n",
      "S-micro score: 0.1476573592049219\n"
     ]
    }
   ],
   "source": [
    "real, preds = calculate_metrics(test_dataset, ner_model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0680ff-1a8c-4bf8-a784-5739831af79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
