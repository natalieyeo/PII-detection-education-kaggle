{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33f9a79-f86f-4852-9e3f-0958da1b61a1",
   "metadata": {},
   "source": [
    "**Presidio Analyzer**\n",
    "\n",
    "After some data exploration, we test out Microsoft Presidio Analyzer's performance in recognizing named entities in the text. Microsoft Presidio Analyzer is a Named Entity Recognizer that combines both rules-based pattern recognition with machine learning techniques to detect sensitive information. It is pre-trained and \"provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\" (https://microsoft.github.io/presidio/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d29d46-387e-4559-bfbb-291f0837bf98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://microsoft.github.io/presidio/assets/detection_flow.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'https://microsoft.github.io/presidio/assets/detection_flow.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c29f3f-cc2e-4622-858c-9f4f61a095b5",
   "metadata": {},
   "source": [
    "The named entities labelled in the Presidio Learning Agency PII dataset differ from the supported named entities, so we will adapt Presidio's base recognition capabilities and enhance them with custom, rule-based entitities. We also created a signal phrase recognizer to try to facilitate the model in distinguishing the name of student writer from the names of other people the student might be citing in the work.\n",
    "\n",
    "These adaptations to Presidio Analyzer yield a F5 score of 0.77, which indicates high recall. This means that the model makes a lot of guesses, but most of them are wrong (relatively high number of false positives to the true positives). However, making a lot of guesses means that the model does not miss out on classifying PII labels (comparatively low number of false negatives). Out of the 2739 PII labels, the enhanced Presidio Analyzer model correctly labels 2256, which means it has a relatively high level of accuracy, given the large imbalance in positive and negative classification (only 2739 out of nearly 5 million word tokens are labelled as PII). \n",
    "\n",
    "The results for this model will set up a baseline to which we will compare deep learning models in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cdcf73-9333-46a7-adad-b61cff10ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import spacy\n",
    "spacy.load('en_core_web_lg')\n",
    "from presidio_analyzer import Pattern, PatternRecognizer, RecognizerResult, AnalyzerEngine\n",
    "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts, NlpEngineProvider\n",
    "from presidio_analyzer.predefined_recognizers import EmailRecognizer, UrlRecognizer, PhoneRecognizer\n",
    "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from bisect import bisect_left\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras.utils import pad_sequences\n",
    "from conlleval import evaluate\n",
    "import sklearn\n",
    "import keras_nlp\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "train = pd.read_json('data/train.json')\n",
    "test = pd.read_json('data/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ff218-56f9-4cce-a255-f7c58b39a6cf",
   "metadata": {},
   "source": [
    "**Functions to Process Data for Presidio Analyzer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0b951-4e41-48fb-b5c2-8732d00d0bd7",
   "metadata": {},
   "source": [
    "These functions convert word and sentence tokens into start and end indicies in the full text. This is helpful because Presidio Analyzer takes in the full text and outputs entries of named entities accompanied by the start and end indices of the entities. Having the indices of the sentences is also helpful so that we can add sentence-based rules to boost the performance of Presidio Analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c16ad91-5c7f-48eb-888c-02f6d32fab60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a000a8d9d24fdcb5e4fbbaa51bdd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function to convert token words as indices of the full text\n",
    "def convert_word_tokens_to_index(row):\n",
    "    tokens = row['tokens']\n",
    "    start_ind = []\n",
    "    end_ind = []\n",
    "    prev_ind = 0\n",
    "    for tok in tokens:\n",
    "        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n",
    "        end = start + len(tok)\n",
    "        start_ind.append(start)\n",
    "        end_ind.append(end)\n",
    "        prev_ind = end\n",
    "    return start_ind, end_ind\n",
    "\n",
    "indexed_text = pd.DataFrame(train.apply(func = convert_word_tokens_to_index, axis = 1))\n",
    "\n",
    "#Create columns of start and end indices to add to the training dataframe\n",
    "train['word_start_ind'] = ''\n",
    "train['word_end_ind'] = ''\n",
    "for row_num in tqdm(range(len(indexed_text))):\n",
    "    train.at[row_num,'word_start_ind'] = indexed_text.iloc[row_num,0][0]\n",
    "    train.at[row_num,'word_end_ind'] = indexed_text.iloc[row_num,0][1]\n",
    "\n",
    "\n",
    "#Checking that the length of the index list equals the length of the tokens and labels lists in the first row\n",
    "len(train.word_start_ind[0]) == len(train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf8d5d2-32b8-4384-bead-fc03b2b99496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae701c828b974d0fa5a5c27e4adf6c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create sentence tokens based on the text\n",
    "train['sentence_tokens'] = ''\n",
    "for row_num in tqdm(range(len(train))):\n",
    "    split_text = [x.split('\\n\\n') for x in nltk.sent_tokenize(train.full_text[row_num])]\n",
    "    split_text = [x\n",
    "                for s in split_text\n",
    "                for x in s]\n",
    "    train.at[row_num,'sentence_tokens'] = split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f4d3e7-92fa-44ad-b40d-20cc9eae601a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224ed89769a44b60882f200415c2ef05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_sent_tokens_to_index(row):\n",
    "    tokens = row['sentence_tokens']\n",
    "    start_ind = []\n",
    "    end_ind = []\n",
    "    prev_ind = 0\n",
    "    for tok in tokens:\n",
    "        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n",
    "        end = start + len(tok)\n",
    "        start_ind.append(start)\n",
    "        end_ind.append(end)\n",
    "        prev_ind = end\n",
    "    return start_ind, end_ind\n",
    "\n",
    "indexed_text = pd.DataFrame(train.apply(func = convert_sent_tokens_to_index, axis = 1))\n",
    "train['sent_start_ind'] = ''\n",
    "train['sent_end_ind'] = ''\n",
    "for row_num in tqdm(range(len(indexed_text))):\n",
    "    train.at[row_num, 'sent_start_ind'] = indexed_text.iloc[row_num,0][0]\n",
    "    train.at[row_num,'sent_end_ind'] = indexed_text.iloc[row_num,0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae2e05-2e35-4904-afaf-7e635785381b",
   "metadata": {},
   "source": [
    "**Setting Up Custom Recognizers for Presidio Analyzer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5af1a1-6dfd-4076-b5d4-9bdfdb67f473",
   "metadata": {},
   "source": [
    "We created custom recognizers in Presidio Analyzer using regex. In order to further enhance the model, we added a signal phrases recognizer. The signal phrases in the text file were extracted from the site https://www.yourdictionary.com/articles/examples-signal-phrases. This recognizer will be used in the sentence context to help distinguish between the names of students and the names of authors, because the default for Microsoft Presidio is to classify everyone as a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb324d08-6b4d-42cf-bbdd-3d7ec1354e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('signal_phrases.txt','r') as f:\n",
    "    contents = f.readlines()\n",
    "    f.close()\n",
    "contents = [str(entry).replace(\"\\n\",\"\").split(\"/\") for entry in contents]\n",
    "contents = [x\n",
    "            for i in contents\n",
    "            for x in i]\n",
    "signal_phrases_recognizer = PatternRecognizer(supported_entity = 'SIGNAL_PHRASE', deny_list = contents)\n",
    "\n",
    "id_regex = r'([A-Za-z]{2}[.?]:)?\\d{12,12}'\n",
    "id_pattern = Pattern(name=\"id\", regex=id_regex, score = 0.5)\n",
    "id_recognizer = PatternRecognizer(supported_entity=\"ID_CUSTOM\", patterns = [id_pattern])\n",
    "\n",
    "address_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(cir(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\n",
    "address_pattern = Pattern(name=\"address\", regex=address_regex, score=0.5)\n",
    "address_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern], context=[\"st\", \"Apt\"])\n",
    "\n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "email_pattern = Pattern(name=\"email address\", regex=email_regex, score=0.5)\n",
    "email_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n",
    "\n",
    "url_regex = r'((https?)|(http?)|(ftp?))://\\S+|www\\.\\S+'\n",
    "url_pattern = Pattern(name=\"url\", regex=url_regex, score=0.5)\n",
    "url_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n",
    "\n",
    "phone_regex = r'^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$'\n",
    "phone_pattern = Pattern(name='phone', regex=phone_regex, score=0.5)\n",
    "phone_recognizer = PatternRecognizer(supported_entity='PHONE_CUSTOM', patterns=[phone_pattern])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1204c-9ea8-4d0c-819f-27fcb3541b38",
   "metadata": {},
   "source": [
    "The ALLOW_LIST parameter in Presidio Analyzer allows us to customize which entries should not be classified as entities. During exploratory Presidio Analyzer runs, we found that the model tended to incorrectly classify names of companies and stores as student names. Including stopwords in the corpus also safeguards Presidio Analyzer from classifying stopwords, or commonly used language, as entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b596621e-4f66-493f-8fff-a599459e0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalieyeo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "ALLOW_LIST = []\n",
    "all_stopwords = list(stopwords.words())\n",
    "words = Counter()\n",
    "for doc in train.tokens:\n",
    "    words.update(doc)\n",
    "for doc in test.tokens:\n",
    "    words.update(doc)\n",
    "all_stopwords  += [str(w).lower() for w, i in words.items() if i > 55]\n",
    "all_stopwords = list(sorted(set(all_stopwords)))\n",
    "del words\n",
    "\n",
    "ALLOW_LIST.extend(all_stopwords)\n",
    "ALLOW_LIST = [word for word in ALLOW_LIST if word not in contents]\n",
    "\n",
    "#S&P 500 companies dataset taken from https://github.com/datasets/s-and-p-500-companies/tree/main/data\n",
    "sp500 = pd.read_csv('data/names/companies-sp.csv')['Security'].str.upper().tolist()\n",
    "famous_brands = ['FACEBOOK','GOOGLE','MICROSOFT','TIKTOK','INSTAGRAM','NETFLIX','DISNEY+','APPLE','TABLEAU','POWER BI','SNOWFLAKE',\n",
    "                 'AZURE','ORACLE','IBM','TED TALK']\n",
    "ALLOW_LIST.extend(famous_brands)\n",
    "ALLOW_LIST.extend(sp500)\n",
    "ALLOW_LIST.extend(['DESIGN','CHALLENGE','COMPETITION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7549247c-53d4-4a7e-862f-274d5afca7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "registry.add_recognizer(address_recognizer)\n",
    "registry.add_recognizer(email_recognizer)\n",
    "registry.add_recognizer(url_recognizer)\n",
    "registry.add_recognizer(phone_recognizer)\n",
    "registry.add_recognizer(id_recognizer)\n",
    "registry.add_recognizer(signal_phrases_recognizer)\n",
    "\n",
    "#Played around with the context aware enhancer settings but didn't add many context phrases\n",
    "#The problem is that the Presidio Model tends to overclassify with a very high amount of false positives\n",
    "#Couldn't conceptualize a way to use the Lemma Context Aware Enhancer to stop the model from classifying something as PII\n",
    "context_aware_enhancer = LemmaContextAwareEnhancer(\n",
    "    context_similarity_factor=0.7, min_score_with_context_similarity=0.6\n",
    ")\n",
    "analyzer = AnalyzerEngine(registry = registry,context_aware_enhancer = context_aware_enhancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d5e589e-7363-4a04-8804-dcba6e660ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions which will be used in conjunction with the Presidio Analyzer function to predict BIO labels\n",
    "from bisect import bisect_right\n",
    "\n",
    "def find_le(a, x):\n",
    "    'Find rightmost value less than or equal to x'\n",
    "    i = bisect_right(a, x)\n",
    "    if i:\n",
    "        return a[i-1]\n",
    "    raise ValueError\n",
    "    \n",
    "def count_trailing_whitespaces(word):\n",
    "    return len(word) - len(word.rstrip())\n",
    "\n",
    "def is_valid_date(text):\n",
    "    try:\n",
    "        parsed_date = parser.parse(text)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def index_to_word_token_num(df, row_num, start, end):\n",
    "    'Convert the index of the full text to the word token list index for a particular row.'\n",
    "    text = df['full_text'][row_num][start:end]\n",
    "    token_start_ind = df.word_start_ind[row_num].index(start)\n",
    "    token_end_ind = df.word_end_ind[row_num].index(end - count_trailing_whitespaces(text))\n",
    "    \n",
    "    return text.rstrip(), token_start_ind, token_end_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cc8c8-81cd-4697-81b5-c941cd066908",
   "metadata": {},
   "source": [
    "**Prediction Function**\n",
    "\n",
    "In the function below, we use Presidio Analyzer recognize a list of supported and custom entities. We apply a bit of data cleaning to make sure that whitespace is not classified as entities, and we also use the signal phrases detected by Presidio Analyzer to weed out person entities that are in the same sentence as the signal phrase and decrease false positives in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4abab9cf-0bfe-4397-a6d3-89c039e6c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(df, row_num):\n",
    "    'Function that takes the dataframe and row number, analyzes entities, and returns a dataframe with document, token number, and label.'\n",
    "    \n",
    "    #Analyze a corpus using Microsoft Presidio\n",
    "    result = analyzer.analyze(text = df['full_text'][row_num],\n",
    "                              entities = [\"PHONE_NUMBER\", \"URL_CUSTOM\", \"EMAIL_ADDRESS\",\"EMAIL_CUSTOM\", \"ADDRESS_CUSTOM\", \"US_SSN\",\n",
    "                                          \"US_ITIN\",\"US_PASSPORT\", \"US_BANK_NUMBER\", \"USERNAME\", \"ID_CUSTOM\",'SIGNAL_PHRASE','PERSON'],\n",
    "                              allow_list=ALLOW_LIST,\n",
    "                              language='en', \n",
    "                              score_threshold=0.5) #Increased score threshold from 0.005 to 0.5\n",
    "    preds = []\n",
    "    label = \"\"\n",
    "    \n",
    "    #Loading compilers for regex patterns\n",
    "    parenthesis = re.compile(\"^\\\\(.*$|^.\\\\)$\")\n",
    "    \n",
    "    starts = []\n",
    "    whitespace = ['\\n\\n', '\\xa0b', ' ']\n",
    "    \n",
    "    for x in result:\n",
    "        if x.start not in df.word_start_ind[row_num]:\n",
    "            break\n",
    "        if x.end not in df.word_end_ind[row_num]:\n",
    "            break\n",
    "        if x.start not in starts: #This allows function to skip over entries that it already labelled\n",
    "            starts.append(x.start)\n",
    "            text, word_token_start, word_token_end = index_to_word_token_num(df, row_num, x.start, x.end)\n",
    "\n",
    "            if x.entity_type == 'PERSON' or x.entity_type == 'PERSON_CUSTOM':\n",
    "                sentence_start = find_le(df.sent_start_ind[row_num], x.start)\n",
    "                sentence_idx = df.sent_start_ind[row_num].index(sentence_start)\n",
    "                sentence_text = df.sentence_tokens[row_num][sentence_idx]\n",
    "                sentences_with_signals = [find_le(df.sent_start_ind[row_num], s.start) \n",
    "                                          for s in result if s.entity_type == 'SIGNAL_PHRASE']\n",
    "                if sentence_start in sentences_with_signals:\n",
    "                    label = \"O\"\n",
    "                elif parenthesis.search(sentence_text):\n",
    "                    label = \"O\"\n",
    "                else:\n",
    "                    label = \"NAME_STUDENT\"\n",
    "            if x.entity_type == 'PHONE_NUMBER':\n",
    "                label = \"PHONE_NUM\"\n",
    "            if x.entity_type == 'EMAIL_ADDRESS' or x.entity_type == 'EMAIL_CUSTOM':\n",
    "                label = \"EMAIL\"\n",
    "            if x.entity_type == 'URL_CUSTOM':\n",
    "                label = 'URL_PERSONAL'\n",
    "            if x.entity_type == 'ADDRESS_CUSTOM':\n",
    "                label = 'STREET_ADDRESS'\n",
    "            if x.entity_type in ['US_SSN', 'US_ITIN', 'US_PASSPORT', 'US_BANK_NUMBER', 'ID_CUSTOM']:\n",
    "                label = 'ID_NUM'\n",
    "            if x.entity_type == 'USERNAME':\n",
    "                label =  'USERNAME'\n",
    "\n",
    "            if (label != \"O\" and x.entity_type != 'SIGNAL_PHRASE'):\n",
    "                labels = ['B-' + label]\n",
    "                if (word_token_end - word_token_start > 0):\n",
    "                    inner_labels = ['I-' + label for i in range((word_token_start + 1), word_token_end + 1)]\n",
    "                    labels.extend(inner_labels)\n",
    "                    if '\\n\\n' in text:\n",
    "                        end_idx = word_token_end + 1\n",
    "                        tab_idx = df.loc[row_num, 'tokens'][word_token_start:end_idx].index('\\n\\n')\n",
    "                        labels[tab_idx] = \"O\"\n",
    "                        labels[tab_idx + 1] = 'B-' + label\n",
    "                token_range = range(word_token_start, word_token_end + 1)\n",
    "                df_result = pd.DataFrame({'document': [df.document[row_num] for i in token_range],\n",
    "                                          'token': list(token_range),\n",
    "                                          'token_text': [df.tokens[row_num][i] for i in token_range],\n",
    "                                          #'sentence_text': [sentence_text for i in token_range],\n",
    "                                          'preds': labels})\n",
    "                df_result = df_result[~df_result.isin(whitespace)]\n",
    "                df_result = df_result[df_result.token_text.isna() == False]\n",
    "                preds.append(df_result)\n",
    "    if len(preds) > 1:\n",
    "        result = pd.concat(preds, axis = 0, ignore_index=True)\n",
    "        if result[['document','token']].duplicated().sum() > 0:\n",
    "            duplicated_tokens = list(result.loc[result[['document','token']].duplicated(),'token'])\n",
    "            for i in duplicated_tokens:\n",
    "                duplicated_labels = result.loc[result.token == i, 'preds']\n",
    "                for j in duplicated_labels.index:\n",
    "                    if 'I-' in result.loc[j, 'preds']:\n",
    "                        result = result.drop(index = j)\n",
    "        return result\n",
    "    elif len(preds) == 1:\n",
    "        return preds[0]\n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d977896d-e595-4bd0-96c9-a4094f2833c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1f8f4443e549e291dd047ca83b5995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a dataframe of the correct labels\n",
    "correct_labels = []\n",
    "\n",
    "for i in tqdm(range(len(train))):\n",
    "    tmp_df = pd.DataFrame({'document' : train.document[i],\n",
    "                           'token': range(len(train.tokens[i])),\n",
    "                           'token_text': train.tokens[i],\n",
    "                           'correct_label': train.labels[i]})\n",
    "    correct_labels.append(tmp_df)\n",
    "\n",
    "correct_df = pd.concat(correct_labels,axis = 0,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6158de-f0f9-4341-b2b1-26a81f7a2787",
   "metadata": {},
   "source": [
    "Below is a function to count the number true positives, false positives, true negatives, and false negatives in the prediction. We use these to calculate metrics like precision, recall, F1 score, and the F beta 5 score that the kaggle competition is using to evaluate the results. F beta 5 score is like the F1 score but puts 5 times as much weight on recall than on precision, indicating that recall is much more important. We don't want the model to miss out on PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da0f3218-93f4-44e6-9443-c3b52880da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(pred_df,correct_df,beta=5):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - pred_df (DataFrame): DataFrame containing predicted PII labels.\n",
    "    - gt_df (DataFrame): DataFrame containing ground truth PII labels.\n",
    "    - beta (float): The beta parameter for the F-beta score, controlling the trade-off between precision and recall.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: prediction DataFrame merged with the ground truth DataFrame\n",
    "    - float: Micro F-beta score.\n",
    "    \"\"\"   \n",
    "    df = correct_df.merge(pred_df.drop('token_text',axis = 1), on = ['document','token'],how = 'left')\n",
    "    df.preds.fillna('O',axis = 0,inplace=True)\n",
    "    df.loc[(df.correct_label == df.preds) & (df.correct_label != \"O\"), 'cm'] = 'TP'\n",
    "    df.loc[(df.correct_label == \"O\") & (df.preds != \"O\"), 'cm'] = 'FP'\n",
    "    df.loc[(df.correct_label != \"O\") & (df.preds == \"O\"), 'cm'] = 'FN'\n",
    "    df.loc[(df.correct_label == \"O\") & (df.preds ==\"O\"),'cm'] = 'TN'\n",
    "\n",
    "    TP = (df['cm'] == 'TP').sum()\n",
    "    FP = (df['cm'] == 'FP').sum()\n",
    "    FN = (df['cm'] == 'FN').sum()\n",
    "    TN = (df['cm'] == 'TN').sum()\n",
    "    total = TP + FP + FN + TN\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = (TP / (TP + FN))\n",
    "    f1 = 2* (precision * recall / (precision + recall))\n",
    "    \n",
    "    print(f'True Positives: {TP}, False Positives: {FP}')\n",
    "    print(f'True Negatives: {TN}, False Negatives: {FN}')\n",
    "    print(f'{len(df) - total} correctly identified as PII but mislabelled.')\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1-Score \" + str(f1))\n",
    "    \n",
    "    s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n",
    "    print(s_micro)\n",
    "    \n",
    "    return df, s_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7f1de-0d70-4ac4-8c88-5d43fe777f1a",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Overall, this rules-based model tends to guess many more false positives than true positives, with much fewer false negatives. This results in a high recall and low precision. The F5 Score is quite high though. And the accuracy of non-O labels is also quite high. The model performs quite well, but at the expense of low precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35261f-7fe7-45bc-94ec-bde729e0a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(len(train))):\n",
    "    predictions.append(predict_label(train, i))\n",
    "\n",
    "round2 = pd.concat(predictions,axis = 0,ignore_index=True)\n",
    "del predictions\n",
    "total_preds_df2, score = fbeta_score(round2, correct_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b12b0338-41cc-4a2a-9077-ec560ffc653a",
   "metadata": {},
   "source": [
    "Round 2 Predictions:\n",
    "True Positives: 2230, False Positives: 5500\n",
    "True Negatives: 4984296, False Negatives: 486\n",
    "30 correctly identified as PII but mislabelled.\n",
    "Precision: 0.2884864165588616\n",
    "Recall: 0.821060382916053\n",
    "F1-Score: 0.4269576871529772\n",
    "F5-Score: 0.7666269998677774"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fe37d-613c-4378-910e-3bab8d837e41",
   "metadata": {},
   "source": [
    "After Round 2, we noticed that some of the labels were being classified twice, so we fixed the predict_labels() function to only allow for one classification. We also prevented white space characters from being identified as PII and increased the Presidio Analyzer score threshold to 0.5 from 0.005. This led to slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6d6a4dc-7d54-4a72-a10f-23c9d250e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/zt7yqxtn7wzf5xtwg4zjzfn80000gn/T/ipykernel_20762/2159591466.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df.preds.fillna('O',axis = 0,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 2256, False Positives: 5411\n",
      "True Negatives: 4984383, False Negatives: 463\n",
      "20 correctly identified as PII but mislabelled.\n",
      "Precision: 0.2942480761706013\n",
      "Recall: 0.8297168076498713\n",
      "F1-Score 0.4344309647602542\n",
      "0.7754422146426588\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(len(train))):\n",
    "    predictions.append(predict_label(train, i))\n",
    "\n",
    "round3 = pd.concat(predictions,axis = 0,ignore_index=True)\n",
    "del predictions\n",
    "total_preds_df3, score = fbeta_score(round3, correct_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb44a941-96c0-4030-b7d6-e238254f1325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F5 Score: 0.7703501352735678\n"
     ]
    }
   ],
   "source": [
    "TP = 2256\n",
    "FN = 483\n",
    "FP = 5411\n",
    "beta = 5\n",
    "s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n",
    "print('Micro F5 Score:', s_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0829abeb-5580-40e1-8c58-2fcfa29551a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13674c4aeb54f42a7f53587582d6395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Function to add sentence context to each token in the dataset\n",
    "from itertools import repeat\n",
    "from bisect import bisect_left\n",
    "def find_lt_idx(a, x):\n",
    "    'Find rightmost value less than x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i:\n",
    "        return i-1\n",
    "\n",
    "def find_sent_token(row):\n",
    "    sentences = row['sentence_tokens']\n",
    "    word_starts = row['word_start_ind']\n",
    "    words = row['tokens']\n",
    "    labels = row['labels']\n",
    "    sentence_starts = row['sent_start_ind']\n",
    "    \n",
    "    sentence_token = [] #Should end up the length of word starts, aka the num of words in the essay\n",
    "    word_tokens_in_sentence = []\n",
    "    labels_in_sentence = []\n",
    "    start_ind = 0\n",
    "    for next_sent_token_num, next_sentence_start in enumerate(sentence_starts[1:],1):\n",
    "        end_ind = find_lt_idx(word_starts,next_sentence_start) + 1\n",
    "        x = end_ind - start_ind\n",
    "        sentence_token[start_ind:end_ind] = repeat(sentences[next_sent_token_num - 1],x)\n",
    "        word_tokens_in_sentence[start_ind:end_ind] = repeat(words[start_ind:end_ind],x)\n",
    "        labels_in_sentence[start_ind:end_ind] = repeat(labels[start_ind:end_ind],x)\n",
    "        start_ind = end_ind\n",
    "    end_ind = len(word_starts)\n",
    "    x = end_ind - start_ind\n",
    "    sentence_token[start_ind:end_ind] = repeat(sentences[len(sentences)-1], x)\n",
    "    word_tokens_in_sentence[start_ind:end_ind] = repeat(words[start_ind:end_ind],x)\n",
    "    labels_in_sentence[start_ind:end_ind] = repeat(labels[start_ind:end_ind],x)\n",
    "    return sentence_token, word_tokens_in_sentence, labels_in_sentence\n",
    "\n",
    "sent_word_conversion = pd.DataFrame(train.apply(func = find_sent_token, axis = 1))\n",
    "\n",
    "train['sent_for_word'] = ''\n",
    "train['words_in_sentence'] = ''\n",
    "train['labels_in_sentence'] = ''\n",
    "for row_num in tqdm(range(len(sent_word_conversion))):\n",
    "    train.at[row_num, 'sent_for_word'] = sent_word_conversion.iloc[row_num,0][0]\n",
    "    train.at[row_num, 'words_in_sentence'] = sent_word_conversion.iloc[row_num,0][1]\n",
    "    train.at[row_num, 'labels_in_sentence'] = sent_word_conversion.iloc[row_num,0][2]\n",
    "\n",
    "\n",
    "total_preds_df3['sentence_text'] = train.sent_for_word.explode().reset_index(drop = True)\n",
    "total_preds_df3['tokenized_sentence'] = train.words_in_sentence.explode().reset_index(drop = True)\n",
    "total_preds_df3['labels_in_sentence'] = train.labels_in_sentence.explode().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ef14a20-6194-44d4-8631-108045760956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>token_text</th>\n",
       "      <th>correct_label</th>\n",
       "      <th>preds</th>\n",
       "      <th>cm</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Design</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Thinking</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>innovation</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992528</th>\n",
       "      <td>22687</td>\n",
       "      <td>815</td>\n",
       "      <td>process</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Despite this, another valid tool that could fi...</td>\n",
       "      <td>[Despite, this, ,, another, valid, tool, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992529</th>\n",
       "      <td>22687</td>\n",
       "      <td>816</td>\n",
       "      <td>explained</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Despite this, another valid tool that could fi...</td>\n",
       "      <td>[Despite, this, ,, another, valid, tool, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992530</th>\n",
       "      <td>22687</td>\n",
       "      <td>817</td>\n",
       "      <td>above</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Despite this, another valid tool that could fi...</td>\n",
       "      <td>[Despite, this, ,, another, valid, tool, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992531</th>\n",
       "      <td>22687</td>\n",
       "      <td>818</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Despite this, another valid tool that could fi...</td>\n",
       "      <td>[Despite, this, ,, another, valid, tool, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992532</th>\n",
       "      <td>22687</td>\n",
       "      <td>819</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>Despite this, another valid tool that could fi...</td>\n",
       "      <td>[Despite, this, ,, another, valid, tool, that,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4992533 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         document  token  token_text correct_label preds  cm  \\\n",
       "0               7      0      Design             O     O  TN   \n",
       "1               7      1    Thinking             O     O  TN   \n",
       "2               7      2         for             O     O  TN   \n",
       "3               7      3  innovation             O     O  TN   \n",
       "4               7      4   reflexion             O     O  TN   \n",
       "...           ...    ...         ...           ...   ...  ..   \n",
       "4992528     22687    815     process             O     O  TN   \n",
       "4992529     22687    816   explained             O     O  TN   \n",
       "4992530     22687    817       above             O     O  TN   \n",
       "4992531     22687    818           .             O     O  TN   \n",
       "4992532     22687    819        \\n\\n             O     O  TN   \n",
       "\n",
       "                                             sentence_text  \\\n",
       "0        Design Thinking for innovation reflexion-Avril...   \n",
       "1        Design Thinking for innovation reflexion-Avril...   \n",
       "2        Design Thinking for innovation reflexion-Avril...   \n",
       "3        Design Thinking for innovation reflexion-Avril...   \n",
       "4        Design Thinking for innovation reflexion-Avril...   \n",
       "...                                                    ...   \n",
       "4992528  Despite this, another valid tool that could fi...   \n",
       "4992529  Despite this, another valid tool that could fi...   \n",
       "4992530  Despite this, another valid tool that could fi...   \n",
       "4992531  Despite this, another valid tool that could fi...   \n",
       "4992532  Despite this, another valid tool that could fi...   \n",
       "\n",
       "                                        tokenized_sentence  \n",
       "0        [Design, Thinking, for, innovation, reflexion,...  \n",
       "1        [Design, Thinking, for, innovation, reflexion,...  \n",
       "2        [Design, Thinking, for, innovation, reflexion,...  \n",
       "3        [Design, Thinking, for, innovation, reflexion,...  \n",
       "4        [Design, Thinking, for, innovation, reflexion,...  \n",
       "...                                                    ...  \n",
       "4992528  [Despite, this, ,, another, valid, tool, that,...  \n",
       "4992529  [Despite, this, ,, another, valid, tool, that,...  \n",
       "4992530  [Despite, this, ,, another, valid, tool, that,...  \n",
       "4992531  [Despite, this, ,, another, valid, tool, that,...  \n",
       "4992532  [Despite, this, ,, another, valid, tool, that,...  \n",
       "\n",
       "[4992533 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_preds_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05a8c20a-0ba4-42b1-a4af-fb6740d5f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_preds_df3.to_json('initial_predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f21e4e5d-4368-4b26-b07b-1b3edceb2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data to a json file to build on this in the next notebook\n",
    "df = pd.read_json('initial_predictions.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
