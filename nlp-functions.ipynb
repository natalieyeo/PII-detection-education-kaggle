{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5f6adb-ccc5-4043-9023-1f88925bab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from bisect import bisect_left\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "import keras\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "from keras import ops\n",
    "from keras.utils import pad_sequences\n",
    "import sklearn\n",
    "from keras import layers\n",
    "import keras_nlp\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "train = pd.read_json('data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0aa589-ed4b-4f2d-9e27-407345ca6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"NAME_STUDENT\", \"EMAIL\", \"URL_PERSONAL\", \"ID_NUM\",'USERNAME','PHONE_NUM','STREET_ADDRESS']\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = ['[PAD]',\"O\"] + all_labels\n",
    "    return dict(zip(all_labels,range(0, len(all_labels) + 1)))\n",
    "\n",
    "encoding = make_tag_lookup_table()\n",
    "mapping = dict([(value, key) for key, value in encoding.items()])\n",
    "\n",
    "#print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d693feca-67aa-4f29-9410-892a800fc3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53985\n"
     ]
    }
   ],
   "source": [
    "all_tokens = train.tokens.explode().reset_index(drop = True).unique()\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "#print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 50000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "del all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3140fda-dc25-475e-b855-62d40cc7bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(df, test_size, neg_sample_size, rdm_state):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe with a column correct_label\n",
    "    test_size: the fraction of positive samples to leave for testing\n",
    "    neg_sample_size: number of negative samples to include in the dataframe\n",
    "    random_state: set the random state of the sample function to access different samples\n",
    "    returns: sample dataframe and indices of documents to test\n",
    "    \"\"\"\n",
    "    np.random.seed(rdm_state)\n",
    "    idx_pos = df.loc[(df.correct_label != \"O\") | (df.cm.isna()),'sentence_text'].drop_duplicates().index\n",
    "    idx_neg = df.loc[df.correct_label == \"O\", 'sentence_text'].drop_duplicates().index\n",
    "    \n",
    "    df2 = pd.concat([df.iloc[idx_pos,:],df.iloc[idx_neg,:]],axis = 0).reset_index(drop = True)\n",
    "    total_idx = df2.loc[:,'sentence_text'].drop_duplicates().index\n",
    "    df2 = df2.iloc[total_idx,:]\n",
    "    pos_df = df2.loc[df2.correct_label != \"O\",:]\n",
    "    neg_df = df2.loc[df2.correct_label == \"O\",:]\n",
    "    \n",
    "    train_size = 1 - test_size\n",
    "    pos_sample = pos_df.sample(frac = train_size,random_state = rdm_state)\n",
    "    neg_sample = neg_df.sample(neg_sample_size,random_state = rdm_state)\n",
    "    df3 = pd.concat([pos_sample,neg_sample],axis = 0).reset_index(drop = True)\n",
    "    pos_sample_test = pos_df[~pos_df.document.isin(pos_sample.document.unique())]\n",
    "    pos_documents = pos_sample_test.document.unique()\n",
    "    neg_sample_test = neg_df[~neg_df.document.isin(neg_sample.document.unique())]\n",
    "    neg_documents = np.random.choice(neg_df.document.unique(),len(pos_documents))\n",
    "    documents = set(np.append(pos_documents,neg_documents))\n",
    "    return df3, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa019f71-1f9c-4d24-9dfa-7f731df76020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(tokens, labels, test = 0.2, state = 42, batch_size = 32):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(tokens, labels, test_size = test, random_state = state)\n",
    "    \n",
    "    X_train = pad_sequences(X_train.map(lookup_layer))\n",
    "    X_val = pad_sequences(X_val.map(lookup_layer))\n",
    "    y_train = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_train]))\n",
    "    y_val = pad_sequences(pd.Series([[encoding[r] for r in row] for row in y_val]))\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val,y_val)).batch(batch_size)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2ae85-07d4-4859-a1c6-614aa9c87c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(tokens, labels, batch_size = 32):\n",
    "\n",
    "    X = pad_sequences(tokens.map(lookup_layer))\n",
    "    y = pad_sequences(pd.Series([[encoding[r] for r in row] for row in labels]))\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,y)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b83ddd-ace2-4dbb-84c4-590886ac777f",
   "metadata": {},
   "source": [
    "**Named Entity Recognition Transformer**\n",
    "\n",
    "Built on code from https://keras.io/examples/nlp/ner_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "799defe3-6904-4207-91ab-1b4b983b7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5530ea3e-1c24-4f55-b4a2-82447722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4680446c-4427-4a98-b136-6cf7ac17a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=3298, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9416c9-7bdd-4e1a-a5e7-d77381c9ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockWithLSTM(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Bidirectional(layers.LSTM(ff_dim, return_sequences=True)),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d7dffb-583a-4441-8fd5-370d162fbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModelWithLSTM(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=3298, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlockWithLSTM(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b1d660-f142-4bd3-bf42-22bf403fec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockWithOneLSTM(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.LSTM(ff_dim, return_sequences=True),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51511c2-9585-44e9-809c-24bdfc322284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModelWithOneLSTM(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=3298, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlockWithOneLSTM(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9bff2729-a468-4e6e-bb16-cb279dfa24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=None\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = ops.cast((y_true > 0), dtype=\"float32\")\n",
    "        loss = loss * mask\n",
    "        return ops.sum(loss) / ops.sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cae59-497a-42b6-850f-928da5e5c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordDataFrame(pd.DataFrame):\n",
    "    '''Function to utilize when adding model results to a dataframe'''\n",
    "    def __init__(self):\n",
    "        super().__init__(columns = ['Model Description', 'Weighted Precision','Weighted Recall', 'Weighted F1',\n",
    "                        'Micro F5','True Positive', 'False Negative', 'False Positive', 'Sample Size','Epochs',\n",
    "                        'Sample Random State','Batch Size','Num Transformer Heads',\n",
    "                        'ff_dim','embed_dim','Dataset Type'])\n",
    "        self.row_num = self.shape[0] - 1\n",
    "        \n",
    "    def add_record(self, dict_info):\n",
    "        new_row = self.shape[0]\n",
    "        for key in dict_info.keys():\n",
    "            self.at[new_row, key] = dict_info[key]\n",
    "\n",
    "    def clear_record(self):\n",
    "        if self.row_num > -1:\n",
    "            self.drop(index = self.row_num, axis = 0, inplace = True)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def clear_df(self):\n",
    "        self.drop(index = self.index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4417290d-aafd-48a5-bfe1-eddebc21260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x[1] for x in list(mapping.items())][2:]\n",
    "def calculate_metrics(dataset, model, beta = 5):\n",
    "    '''This function takes a tensorflow dataset and returns a dataframe of individual results\n",
    "    for the model run as well as a dictionary to use to update the RecordDataFrame().\n",
    "    '''\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = model.predict(x, verbose=0)\n",
    "        predictions = ops.argmax(output, axis=-1)\n",
    "        predictions = ops.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = ops.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    print(f'processed {len(predicted_tags)} tokens')\n",
    "\n",
    "    cm = sklearn.metrics.multilabel_confusion_matrix(real_tags,predicted_tags, labels = labels)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    f5 = []\n",
    "    total = []\n",
    "    true_positive = []\n",
    "    false_positive = []\n",
    "    false_negative = []\n",
    "    total_tp = 0\n",
    "    total_fn = 0\n",
    "    total_fp = 0\n",
    "    for i in range(len(labels)):\n",
    "        tn = cm[i][0][0]\n",
    "        fp = cm[i][0][1]\n",
    "        total_fp += fp\n",
    "        fn = cm[i][1][0]\n",
    "        total_fn += fn\n",
    "        tp = cm[i][1][1]\n",
    "        total_tp += tp\n",
    "        number = fn + tp\n",
    "        if (tp + fp) > 0:\n",
    "            p = tp / (tp + fp)\n",
    "        else: \n",
    "            p = 0\n",
    "        if (tp + fn) > 0:\n",
    "            r = tp / (tp + fn)\n",
    "        else: \n",
    "            r = 0\n",
    "        if (p + r) > 0:    \n",
    "            f = 2 * (p * r / (p + r))\n",
    "        else:\n",
    "            f = 0\n",
    "            \n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "        total.append(number)\n",
    "        true_positive.append(tp)\n",
    "        false_positive.append(fp)\n",
    "        false_negative.append(fn)\n",
    "\n",
    "    result_df = pd.DataFrame({'labels' : labels,\n",
    "                            'precision': precision,\n",
    "                            'recall' : recall,\n",
    "                            'f1' : f1,\n",
    "                            'total': total,\n",
    "                            'TP' : true_positive,\n",
    "                            'FP': false_positive,\n",
    "                            'FN': false_negative})\n",
    "                    \n",
    "    micro_f5 = (1+(beta**2))*total_tp/(((1+(beta**2))*total_tp) + ((beta**2)*total_fn) + total_fp)\n",
    "    weighted_precision = sum(result_df.precision * result_df.total / sum(result_df.total))\n",
    "    weighted_recall = sum(result_df.recall * result_df.total / sum(result_df.total))\n",
    "    weighted_f1 = sum(result_df.f1 * result_df.total / sum(result_df.total))\n",
    "    print(f'True Positive: {total_tp}, False Positive: {total_fp}, False Negative: {total_fn}')\n",
    "    print(f'Weighted Precision: {weighted_precision}')\n",
    "    print(f'Weighted Recall: {weighted_recall}')\n",
    "    print(f'Weighted F1: {weighted_f1}')\n",
    "    print(f'Micro F5: {micro_f5}')\n",
    "    record_dict = {'Weighted Precision': weighted_precision,\n",
    "                   'Weighted Recall': weighted_recall, \n",
    "                   'Weighted F1': weighted_f1,\n",
    "                   'Micro F5': micro_f5,\n",
    "                   'True Positive': total_tp, \n",
    "                   'False Negative': total_fn, \n",
    "                   'False Positive': total_fp, \n",
    "                   'Sample Size': len(real_tags)}\n",
    "    return result_df, record_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
